{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modulo 24 - Combinação de modelos II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarefa 01 - cite 5 diferenças entre adaboost e GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Forma de atualização de pesos:\n",
    "    - O Adaboost ajusta os pesos das amostras dando mais enfase as que foram classificadas incorretamente em cada iteração\n",
    "    - O GBM não repesa explicitamente as amostras mas ajusta os modelos sucessivos com base nos residuos\n",
    "2. Função de perda utilizada:\n",
    "    - O Adaboost utiliza função de perda que penaliza fortemente os erros para assim fazer a atualização dos pesos\n",
    "    - O GBM permite o uso de diversas funções de perda oferecendo maior flexibilidade pra resolver problemas\n",
    "3. Abordagem de otimização:\n",
    "    - O adaboost opera ajustando os pesos das instancias sem utilizar metodos de descida do gradiente.\n",
    "    - O GBM utiliza a descida do gradiente para minimizar a função de perda\n",
    "4. Robustez a outliers:\n",
    "    - O adaboost é mais sensivel pois os outliers podem receber pesos muito altos e impactar siginificativamente o modelo\n",
    "    - O GBM é menos sensivel a outliers principalmente se a função de perda for mais robusta ou se são aplicadas tecnicas de regularização\n",
    "5. Flexibilidade e aplicabilidade:\n",
    "    - O Adaboost tem uma abordagem mais restrita geralmente utilizando classificadores fracos e com função de perda exponencial\n",
    "    - O GBM tem mais flexibilidade podendo ser usado para regressão, classificação e até mesmo problemas de outros tipos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarefa 02 - Acesse o link Scikit-learn – GBM, leia a explicação e crie um jupyter notebook contendo o exemplo de classificação e de regressão do GBM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classficação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.913"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regressão:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43848663277068134"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = make_regression(random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=0)\n",
    "reg = GradientBoostingRegressor(random_state=0)\n",
    "reg.fit(X_train, y_train)\n",
    "reg.predict(X_test[1:2])\n",
    "reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarefa 3 - cite 5 hiperparâmetros importantes no GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **n_estimators:**\n",
    "    - Define quantas arvores serão construidas\n",
    "2. **learning_rate:**\n",
    "    - Controla o quanto cada arvore contribui para o modelo final\n",
    "3. **max_depth:**\n",
    "    - Limita a profundiade maxima de cada arvore\n",
    "4. **subsample:**\n",
    "    - Define a fração de amostras que serão utilizadas para treinar cada árvore\n",
    "5. **max_features:**\n",
    "    - Define o número de variáveis a serem consideradas quando se procura a melhor divisão em cada nó"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarefa 04 -  Acessando o artigo do Jerome Friedman (Stochastic) e pensando no nome dado ao Stochastic GBM, qual é a maior diferença entre os dois algoritmos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A maior diferença está na forma como os dados são utilizados para construir cada árvore. No GBM tradicional, cada árvore é ajustada utilizando todo o conjunto de dados disponível, enquanto no Stochastic GBM (como proposto por Friedman) cada árvore é construída a partir de um subconjunto aleatório dos dados (normalmente sem reposição). Essa amostragem aleatória introduz uma dose de aleatoriedade que pode ajudar a reduzir o overfitting e melhorar a capacidade de generalização do modelo, especialmente quando se trabalha com conjuntos de dados grandes ou ruidosos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
